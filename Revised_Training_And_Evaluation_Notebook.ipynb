{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "997acfd4-ef9e-4a23-86d1-d7f90a768ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the necessary imports below\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "06ee5040-76a8-4151-ba87-34852c98d226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We are setting up the model, albert-base-v2, below specifically using SequenceClassification and id2label and label2id to go back and forth between labels and their encoding\n",
    "model_name = \"albert-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, id2label={0: \"Non-Limerick\", 1: \"Limerick\", 2: \"5 Lines\", 3: \"Not 5 Lines\", 4: \"AABBA Rhyme Scheme\", 5: \"Not AABBA Rhyme Scheme\"}, label2id={\"Non-Limerick\": 0, \"Limerick\": 1, \"5 Lines\": 2, \"Not 5 Lines\": 3, \"AABBA Rhyme Scheme\": 4, \"Not AABBA Rhyme Scheme\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4e95ad07-49cd-47ad-8437-24f3ad76a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We prepare the dataset by creating encodings specified below for classification labels and reasoning labels, tokenize the input information, etc. below\n",
    "id2label={0: \"Non-Limerick\", 1: \"Limerick\", 2: \"5 Lines\", 3: \"Not 5 Lines\", 4: \"AABBA Rhyme Scheme\", 5: \"Not AABBA Rhyme Scheme\"}\n",
    "encoding_length = len(id2label)\n",
    "encoding_elements = id2label.items()\n",
    "\n",
    "def prepare_dataset(poems, classification_labels, reasoning_labels):\n",
    "    label_encoding = []\n",
    "    for classification_label, reasoning_label in zip(classification_labels, reasoning_labels):\n",
    "        labels = [0] * encoding_length\n",
    "        for index, label in encoding_elements:\n",
    "            if label == classification_label or label in reasoning_label.split(\", \"):\n",
    "                labels[index] = 1\n",
    "        label_encoding.append(labels)\n",
    "    return Dataset.from_dict({\"text\": [f\"Poem:\\n{p}\" for p in poems], \"label\": label_encoding})\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_length=256):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "questions_df = pd.read_csv(\"Copy of Fine_Tuning_Assignment - Limerick Classification.csv\")\n",
    "\n",
    "dataset = prepare_dataset(questions_df[\"Input (Poem)\"], questions_df[\"Label (Limerick or Non-Limerick)\"], questions_df[\"Reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0d6941a9-e40e-4fa6-8897-db916862c4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6825057e-e103-4813-926f-7b3a277b8cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e6f454fef141bb8917d89d8cbe5250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We actually tokenize the dataset and then split the dataset into training and test sets (90-10 split)\n",
    "tokenized_dataset = dataset.map(lambda examples: tokenize_function(examples, tokenizer), batched=True)\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ca0963e4-5150-40a8-9fdf-924fcc58d44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 540\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e719fc60-9497-4e13-ad54-eaa6c8385b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': \"Poem:\\nIf the Limerick's cocktail you 'd quaff,\\nStir nonsense with wit, each a half,\\nAdd a dash of good fun,\\nDrop in a pun-\\nAnd then make a noise like a laugh.\",\n",
       "  'label': [0, 1, 1, 0, 1, 0]},\n",
       " {'text': \"Poem:\\nIf the Limerick's cocktail you 'd quaff,\\nStir nonsense with wit, each a half,\\nAdd a dash of good fun,\\nDrop in a pun-\\nAnd then make a noise like a laugh.\",\n",
       "  'label': [0, 1, 1, 0, 1, 0],\n",
       "  'input_ids': [2,\n",
       "   4629,\n",
       "   45,\n",
       "   100,\n",
       "   14,\n",
       "   18185,\n",
       "   22,\n",
       "   18,\n",
       "   18816,\n",
       "   42,\n",
       "   13,\n",
       "   22,\n",
       "   43,\n",
       "   7131,\n",
       "   2460,\n",
       "   15,\n",
       "   13216,\n",
       "   13,\n",
       "   16684,\n",
       "   29,\n",
       "   9642,\n",
       "   15,\n",
       "   206,\n",
       "   21,\n",
       "   519,\n",
       "   15,\n",
       "   3547,\n",
       "   21,\n",
       "   8405,\n",
       "   16,\n",
       "   254,\n",
       "   2414,\n",
       "   15,\n",
       "   2804,\n",
       "   19,\n",
       "   21,\n",
       "   11582,\n",
       "   8,\n",
       "   17,\n",
       "   94,\n",
       "   233,\n",
       "   21,\n",
       "   3406,\n",
       "   101,\n",
       "   21,\n",
       "   3051,\n",
       "   9,\n",
       "   3,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]})"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0], tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "745030ab-0017-450c-909e-f3e073390864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use get_predictions to take in logits and then specifically set the indices corresponding to the highest value in the classification \n",
    "# indices (0, 1) to 1 and then the indices corresponding to the two highest values in the reasoning indices (2, 3, 4, 5) to 1 \n",
    "# and then return this.\n",
    "classification_indices = [0, 1]\n",
    "reasoning_indices = [2, 3, 4, 5]\n",
    "\n",
    "def get_predictions(input):\n",
    "    input_shape = input.shape\n",
    "    output = np.zeros(input_shape)\n",
    "    for i in range(len(input)):\n",
    "        top_class_index = np.argmax(input[i, classification_indices])\n",
    "        output[i, classification_indices[top_class_index]] = 1\n",
    "    for i in range(len(input)):\n",
    "        reasoning_values = input[i, reasoning_indices]\n",
    "        top_two_reasoning_indices = np.argsort(reasoning_values)[-2:]\n",
    "        for index in top_two_reasoning_indices:\n",
    "            output[i, reasoning_indices[index]] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8ee5cbe2-e1bd-4a6d-b237-635a214280d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute classification metrics (accuracy, precision, recall, and f1) using the sklearn library for both classification and reasoning parts\n",
    "def compute_metrics(predictions):\n",
    "    all_logits, all_labels = predictions\n",
    "    final_predictions = get_predictions(all_logits)\n",
    "    final_predictions = final_predictions.astype(int)\n",
    "    all_labels = all_labels.astype(int)\n",
    "    \n",
    "    classification_predictions = [np.argmax(row[classification_indices]) for row in final_predictions]\n",
    "    classification_labels = [np.argmax(row[classification_indices]) for row in all_labels]\n",
    "    classification_results = {\n",
    "        \"Classification Accuracy\": accuracy_score(classification_labels, classification_predictions),\n",
    "        \"Classification Precision\": precision_score(classification_labels, classification_predictions, average=\"binary\", zero_division=0),\n",
    "        \"Classification Recall\": recall_score(classification_labels, classification_predictions, average=\"binary\", zero_division=0),\n",
    "        \"Classification F1\": f1_score(classification_labels, classification_predictions, average=\"binary\", zero_division=0)\n",
    "    }\n",
    "\n",
    "    reasoning_predictions = final_predictions[:, reasoning_indices].flatten()\n",
    "    reasoning_labels = all_labels[:, reasoning_indices].flatten()\n",
    "    reasoning_results = {\n",
    "        \"Reasoning Accuracy\": accuracy_score(reasoning_labels, reasoning_predictions),\n",
    "        \"Reasoning Precision\": precision_score(reasoning_labels, reasoning_predictions, average=\"micro\", zero_division=0),\n",
    "        \"Reasoning Recall\": recall_score(reasoning_labels, reasoning_predictions, average=\"micro\", zero_division=0),\n",
    "        \"Reasoning F1\": f1_score(reasoning_labels, reasoning_predictions, average=\"micro\", zero_division=0)\n",
    "    }\n",
    "\n",
    "    return {**classification_results, **reasoning_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bca2782b-2742-49d8-a50d-50c0c7f77009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a custom trainer for binary class and multi-label/reason classification with weighted loss computations.\n",
    "classification_indices = [0, 1]\n",
    "reasoning_indices = [2, 3, 4, 5]\n",
    "\n",
    "class BinaryClassMultiLabelTrainer(Trainer):\n",
    "    def __init__(self, weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weights = weights\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        target_labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        predicted_logits = outputs[0]\n",
    "        classification_loss = torch.nn.functional.cross_entropy(predicted_logits[:, classification_indices], target_labels[:, classification_indices])\n",
    "        reasoning_loss = torch.nn.functional.binary_cross_entropy_with_logits(predicted_logits[:, reasoning_indices], target_labels[:, reasoning_indices])\n",
    "        loss = self.weights[0] * classification_loss + self.weights[1] * reasoning_loss\n",
    "        if return_outputs:\n",
    "            return (loss, outputs)\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "28097cff-3984-49f0-aa7a-c2c8240d7649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnapalempalli/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "output_dir = \"./fine_tuned_albert\"\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "30676f2b-87cb-45f2-985a-2809b18aef22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 02:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Classification accuracy</th>\n",
       "      <th>Classification precision</th>\n",
       "      <th>Classification recall</th>\n",
       "      <th>Classification f1</th>\n",
       "      <th>Reasoning accuracy</th>\n",
       "      <th>Reasoning precision</th>\n",
       "      <th>Reasoning recall</th>\n",
       "      <th>Reasoning f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.065600</td>\n",
       "      <td>2.333874</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.741667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.632500</td>\n",
       "      <td>1.561169</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.608700</td>\n",
       "      <td>1.516489</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.226500</td>\n",
       "      <td>1.444254</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.858333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.166100</td>\n",
       "      <td>1.487968</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.841667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 1.4879677295684814, 'eval_Classification Accuracy': 0.7333333333333333, 'eval_Classification Precision': 0.6666666666666666, 'eval_Classification Recall': 0.72, 'eval_Classification F1': 0.6923076923076923, 'eval_Reasoning Accuracy': 0.8416666666666667, 'eval_Reasoning Precision': 0.8416666666666667, 'eval_Reasoning Recall': 0.8416666666666667, 'eval_Reasoning F1': 0.8416666666666667, 'eval_runtime': 1.1682, 'eval_samples_per_second': 51.36, 'eval_steps_per_second': 6.848, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# We initialize trainer\n",
    "trainer = BinaryClassMultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_test[\"train\"],\n",
    "    eval_dataset=train_test[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    weights=(0.7, 4)\n",
    ")\n",
    "trainer.can_return_loss = True\n",
    "\n",
    "# We start training\n",
    "trainer.train()\n",
    "\n",
    "# We save the trained model and evaluate the results\n",
    "trainer.save_model(\"./fine_tuned_albert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_albert\")\n",
    "\n",
    "test_results = trainer.evaluate()\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2aa0b60d-fe55-429e-978f-9a694ee58272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Predictions:\n",
      "Input: Poem:\n",
      "    A cannibal monarch imperial\n",
      "    Kept his wives on a diet of cereal,\n",
      "    But he didn't much care\n",
      "    What the women should wear,\n",
      "    Nor did they; it was quite immaterial.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a foppish old beau,\n",
      "    Who said, \"I find walking too sleau.\n",
      "    So I prances down the street\n",
      "    And throw out my feet\n",
      "    And trip my fantastical teau.\"\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a young maid from Japan\n",
      "    Who married a Hottentot man.\n",
      "    The girl she was yellow.\n",
      "    And black was the fellow.\n",
      "    And their children were all black and tan.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a poor fellow from Lynn,\n",
      "    By accident sat on a pynn,\n",
      "    He let out a shriek,\n",
      "    A howl and a squiek.\n",
      "    And his language was really a synn.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Professor, you should be commended\n",
      "    On your theory so geniusly splendid.\n",
      "    But some say it's luck,\n",
      "    And you really just suck,\n",
      "    'Cause your theory's not what you intended!\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a classical theory\n",
      "    Of which quantum disciples were leery.\n",
      "    They said, ‚ÄúWhy spend so long\n",
      "    On a theory that‚Äôs wrong?‚Äù\n",
      "    Well, it works for your everyday query!\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Consider, when seeking gestalts,\n",
      "    The theories that science exalts.\n",
      "    It's not that they're known\n",
      "    To be written in stone.\n",
      "    It's just that we can't say they're false.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    We need to take care of the one world we live in!\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    In familiar bed,\n",
      "    hands reaching into the light.\n",
      "    Soul blossoms tonight.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Prayers are good wishes\n",
      "    rising up to the realm of\n",
      "    possibilities.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Once more the storm is howling, and half hid\n",
      "    Under this cradle-hood and coverlid\n",
      "    My child sleeps on. There is no obstacle\n",
      "    But Gregory's wood and one bare hill\n",
      "    Whereby the haystack- and roof-levelling wind,\n",
      "    Bred on the Atlantic, can be stayed;\n",
      "    And for an hour I have walked and prayed\n",
      "    Because of the great gloom that is in my mind.\n",
      "    I have walked and prayed for this young child an hour\n",
      "    And heard the sea-wind scream upon the tower,\n",
      "    And under the arches of the bridge, and scream\n",
      "    In the elms above the flooded stream;\n",
      "    Imagining in excited reverie\n",
      "    That the future years had come,\n",
      "    Dancing to a frenzied drum,\n",
      "    Out of the murderous innocence of the sea.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    May she be granted beauty and yet not\n",
      "    Beauty to make a stranger's eye distraught,\n",
      "    Or hers before a looking-glass, for such,\n",
      "    Being made beautiful overmuch,\n",
      "    Consider beauty a sufficient end,\n",
      "    Lose natural kindness and maybe\n",
      "    The heart-revealing intimacy\n",
      "    That chooses right, and never find a friend.\n",
      "    Helen being chosen found life flat and dull\n",
      "    And later had much trouble from a fool,\n",
      "    While that great Queen, that rose out of the spray,\n",
      "    Being fatherless could have her way\n",
      "    Yet chose a bandy-legg√®d smith for man.\n",
      "    It's certain that fine women eat\n",
      "    A crazy salad with their meat\n",
      "    Whereby the Horn of Plenty is undone.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    A cannibal monarch\n",
      "    Kept his wives on a diet,\n",
      "    But he didn't much care\n",
      "    What the women should look like\n",
      "    Nor did they; it was quite immaterial.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a poor fellow,\n",
      "    By accident sat on a pynn,\n",
      "    He yelled out loud,\n",
      "    A howl and a squiek.\n",
      "    And his language was really a curse.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Fine-Tuned Model Predictions:\n",
      "Input: Poem:\n",
      "    A cannibal monarch imperial\n",
      "    Kept his wives on a diet of cereal,\n",
      "    But he didn't much care\n",
      "    What the women should wear,\n",
      "    Nor did they; it was quite immaterial.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a foppish old beau,\n",
      "    Who said, \"I find walking too sleau.\n",
      "    So I prances down the street\n",
      "    And throw out my feet\n",
      "    And trip my fantastical teau.\"\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a young maid from Japan\n",
      "    Who married a Hottentot man.\n",
      "    The girl she was yellow.\n",
      "    And black was the fellow.\n",
      "    And their children were all black and tan.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a poor fellow from Lynn,\n",
      "    By accident sat on a pynn,\n",
      "    He let out a shriek,\n",
      "    A howl and a squiek.\n",
      "    And his language was really a synn.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Professor, you should be commended\n",
      "    On your theory so geniusly splendid.\n",
      "    But some say it's luck,\n",
      "    And you really just suck,\n",
      "    'Cause your theory's not what you intended!\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a classical theory\n",
      "    Of which quantum disciples were leery.\n",
      "    They said, ‚ÄúWhy spend so long\n",
      "    On a theory that‚Äôs wrong?‚Äù\n",
      "    Well, it works for your everyday query!\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Consider, when seeking gestalts,\n",
      "    The theories that science exalts.\n",
      "    It's not that they're known\n",
      "    To be written in stone.\n",
      "    It's just that we can't say they're false.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    We need to take care of the one world we live in!\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    In familiar bed,\n",
      "    hands reaching into the light.\n",
      "    Soul blossoms tonight.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Prayers are good wishes\n",
      "    rising up to the realm of\n",
      "    possibilities.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Once more the storm is howling, and half hid\n",
      "    Under this cradle-hood and coverlid\n",
      "    My child sleeps on. There is no obstacle\n",
      "    But Gregory's wood and one bare hill\n",
      "    Whereby the haystack- and roof-levelling wind,\n",
      "    Bred on the Atlantic, can be stayed;\n",
      "    And for an hour I have walked and prayed\n",
      "    Because of the great gloom that is in my mind.\n",
      "    I have walked and prayed for this young child an hour\n",
      "    And heard the sea-wind scream upon the tower,\n",
      "    And under the arches of the bridge, and scream\n",
      "    In the elms above the flooded stream;\n",
      "    Imagining in excited reverie\n",
      "    That the future years had come,\n",
      "    Dancing to a frenzied drum,\n",
      "    Out of the murderous innocence of the sea.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    May she be granted beauty and yet not\n",
      "    Beauty to make a stranger's eye distraught,\n",
      "    Or hers before a looking-glass, for such,\n",
      "    Being made beautiful overmuch,\n",
      "    Consider beauty a sufficient end,\n",
      "    Lose natural kindness and maybe\n",
      "    The heart-revealing intimacy\n",
      "    That chooses right, and never find a friend.\n",
      "    Helen being chosen found life flat and dull\n",
      "    And later had much trouble from a fool,\n",
      "    While that great Queen, that rose out of the spray,\n",
      "    Being fatherless could have her way\n",
      "    Yet chose a bandy-legg√®d smith for man.\n",
      "    It's certain that fine women eat\n",
      "    A crazy salad with their meat\n",
      "    Whereby the Horn of Plenty is undone.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    A cannibal monarch\n",
      "    Kept his wives on a diet,\n",
      "    But he didn't much care\n",
      "    What the women should look like\n",
      "    Nor did they; it was quite immaterial.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a poor fellow,\n",
      "    By accident sat on a pynn,\n",
      "    He yelled out loud,\n",
      "    A howl and a squiek.\n",
      "    And his language was really a curse.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are setting up the base version of the same model without the fine tuning for comparison purposes\n",
    "model_name = \"albert-base-v2\"\n",
    "finetuned_model_path = \"./fine_tuned_albert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# We test the model on new examples that were not in our dataset\n",
    "new_test_examples = [\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    A cannibal monarch imperial\n",
    "    Kept his wives on a diet of cereal,\n",
    "    But he didn't much care\n",
    "    What the women should wear,\n",
    "    Nor did they; it was quite immaterial.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    There once was a foppish old beau,\n",
    "    Who said, \"I find walking too sleau.\n",
    "    So I prances down the street\n",
    "    And throw out my feet\n",
    "    And trip my fantastical teau.\"''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    There was a young maid from Japan\n",
    "    Who married a Hottentot man.\n",
    "    The girl she was yellow.\n",
    "    And black was the fellow.\n",
    "    And their children were all black and tan.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    There was a poor fellow from Lynn,\n",
    "    By accident sat on a pynn,\n",
    "    He let out a shriek,\n",
    "    A howl and a squiek.\n",
    "    And his language was really a synn.''',\n",
    "    #Limerick\n",
    "    '''Poem:\n",
    "    Professor, you should be commended\n",
    "    On your theory so geniusly splendid.\n",
    "    But some say it's luck,\n",
    "    And you really just suck,\n",
    "    'Cause your theory's not what you intended!''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    There once was a classical theory\n",
    "    Of which quantum disciples were leery.\n",
    "    They said, ‚ÄúWhy spend so long\n",
    "    On a theory that‚Äôs wrong?‚Äù\n",
    "    Well, it works for your everyday query!''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    Consider, when seeking gestalts,\n",
    "    The theories that science exalts.\n",
    "    It's not that they're known\n",
    "    To be written in stone.\n",
    "    It's just that we can't say they're false.''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    We need to take care of the one world we live in!''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    In familiar bed,\n",
    "    hands reaching into the light.\n",
    "    Soul blossoms tonight.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    Prayers are good wishes\n",
    "    rising up to the realm of\n",
    "    possibilities.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    Once more the storm is howling, and half hid\n",
    "    Under this cradle-hood and coverlid\n",
    "    My child sleeps on. There is no obstacle\n",
    "    But Gregory's wood and one bare hill\n",
    "    Whereby the haystack- and roof-levelling wind,\n",
    "    Bred on the Atlantic, can be stayed;\n",
    "    And for an hour I have walked and prayed\n",
    "    Because of the great gloom that is in my mind.\n",
    "    I have walked and prayed for this young child an hour\n",
    "    And heard the sea-wind scream upon the tower,\n",
    "    And under the arches of the bridge, and scream\n",
    "    In the elms above the flooded stream;\n",
    "    Imagining in excited reverie\n",
    "    That the future years had come,\n",
    "    Dancing to a frenzied drum,\n",
    "    Out of the murderous innocence of the sea.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    May she be granted beauty and yet not\n",
    "    Beauty to make a stranger's eye distraught,\n",
    "    Or hers before a looking-glass, for such,\n",
    "    Being made beautiful overmuch,\n",
    "    Consider beauty a sufficient end,\n",
    "    Lose natural kindness and maybe\n",
    "    The heart-revealing intimacy\n",
    "    That chooses right, and never find a friend.\n",
    "    Helen being chosen found life flat and dull\n",
    "    And later had much trouble from a fool,\n",
    "    While that great Queen, that rose out of the spray,\n",
    "    Being fatherless could have her way\n",
    "    Yet chose a bandy-legg√®d smith for man.\n",
    "    It's certain that fine women eat\n",
    "    A crazy salad with their meat\n",
    "    Whereby the Horn of Plenty is undone.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    A cannibal monarch\n",
    "    Kept his wives on a diet,\n",
    "    But he didn't much care\n",
    "    What the women should look like\n",
    "    Nor did they; it was quite immaterial.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There was a poor fellow,\n",
    "    By accident sat on a pynn,\n",
    "    He yelled out loud,\n",
    "    A howl and a squiek.\n",
    "    And his language was really a curse.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works'''\n",
    "]\n",
    "\n",
    "id2label={0: \"Non-Limerick\", 1: \"Limerick\", 2: \"5 Lines\", 3: \"Not 5 Lines\", 4: \"AABBA Rhyme Scheme\", 5: \"Not AABBA Rhyme Scheme\"}\n",
    "label2id={\"Non-Limerick\": 0, \"Limerick\": 1, \"5 Lines\": 2, \"Not 5 Lines\": 3, \"AABBA Rhyme Scheme\": 4, \"Not AABBA Rhyme Scheme\": 5}\n",
    "\n",
    "# We need to have the model make the predictions and then return these predictions as output\n",
    "def decode_predictions(model, tokenizer, new_test_examples, id2label):\n",
    "    tokenized_input = tokenizer(new_test_examples, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    predictions = get_predictions(model(**tokenized_input).logits.detach().numpy())\n",
    "    decoded_predictions = []\n",
    "    for row in predictions:\n",
    "        decoded_row = []\n",
    "        for i, label in enumerate(row):\n",
    "            if label == 1:\n",
    "                decoded_row.append(id2label[i])\n",
    "        decoded_predictions.append(decoded_row)\n",
    "    return decoded_predictions\n",
    "\n",
    "# Our baseline model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, id2label=id2label, label2id=label2id)\n",
    "# Our finetuned model\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "\n",
    "# We get predictions for both models\n",
    "base_model_predictions = decode_predictions(base_model, tokenizer, new_test_examples, id2label)\n",
    "finetuned_model_predictions = decode_predictions(finetuned_model, tokenizer, new_test_examples, id2label)\n",
    "\n",
    "# We print the results\n",
    "print(\"Base Model Predictions:\")\n",
    "for text, prediction in zip(new_test_examples, base_model_predictions):\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {prediction}\\n\")\n",
    "\n",
    "print(\"Fine-Tuned Model Predictions:\")\n",
    "for text, prediction in zip(new_test_examples, finetuned_model_predictions):\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "714190c3-5ba9-4513-af87-4065d306bc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Predictions:\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Fine-Tuned Model Predictions:\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are setting up the base version of the same model without the fine tuning for comparison purposes\n",
    "model_name = \"albert-base-v2\"\n",
    "finetuned_model_path = \"./fine_tuned_albert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# We test the model on new examples that were not in our dataset\n",
    "new_test_examples = [\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    #Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "]\n",
    "\n",
    "id2label={0: \"Non-Limerick\", 1: \"Limerick\", 2: \"5 Lines\", 3: \"Not 5 Lines\", 4: \"AABBA Rhyme Scheme\", 5: \"Not AABBA Rhyme Scheme\"}\n",
    "label2id={\"Non-Limerick\": 0, \"Limerick\": 1, \"5 Lines\": 2, \"Not 5 Lines\": 3, \"AABBA Rhyme Scheme\": 4, \"Not AABBA Rhyme Scheme\": 5}\n",
    "\n",
    "# We need to have the model make the predictions and then return these predictions as output\n",
    "def decode_predictions(model, tokenizer, new_test_examples, id2label):\n",
    "    tokenized_input = tokenizer(new_test_examples, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    predictions = get_predictions(model(**tokenized_input).logits.detach().numpy())\n",
    "    decoded_predictions = []\n",
    "    for row in predictions:\n",
    "        decoded_row = []\n",
    "        for i, label in enumerate(row):\n",
    "            if label == 1:\n",
    "                decoded_row.append(id2label[i])\n",
    "        decoded_predictions.append(decoded_row)\n",
    "    return decoded_predictions\n",
    "\n",
    "# Our baseline model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, id2label=id2label, label2id=label2id)\n",
    "# Our finetuned model\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "\n",
    "# We get predictions for both models\n",
    "base_model_predictions = decode_predictions(base_model, tokenizer, new_test_examples, id2label)\n",
    "finetuned_model_predictions = decode_predictions(finetuned_model, tokenizer, new_test_examples, id2label)\n",
    "\n",
    "# We print the results\n",
    "print(\"Base Model Predictions:\")\n",
    "for text, prediction in zip(new_test_examples, base_model_predictions):\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {prediction}\\n\")\n",
    "\n",
    "print(\"Fine-Tuned Model Predictions:\")\n",
    "for text, prediction in zip(new_test_examples, finetuned_model_predictions):\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "368ff69a-9067-43e6-adfb-097214932df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We are setting up the model, google/electra-base-discriminator, below specifically using SequenceClassification and id2label and label2id to go back and forth between labels and their encoding\n",
    "model_name = \"google/electra-base-discriminator\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, id2label={0: \"Non-Limerick\", 1: \"Limerick\", 2: \"5 Lines\", 3: \"Not 5 Lines\", 4: \"AABBA Rhyme Scheme\", 5: \"Not AABBA Rhyme Scheme\"}, label2id={\"Non-Limerick\": 0, \"Limerick\": 1, \"5 Lines\": 2, \"Not 5 Lines\": 3, \"AABBA Rhyme Scheme\": 4, \"Not AABBA Rhyme Scheme\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "155028aa-cdcf-4fe2-8229-d88770c4dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We prepare the dataset by creating encodings specified below for classification labels and reasoning labels, tokenize the input information, etc. below\n",
    "id2label={0: \"Non-Limerick\", 1: \"Limerick\", 2: \"5 Lines\", 3: \"Not 5 Lines\", 4: \"AABBA Rhyme Scheme\", 5: \"Not AABBA Rhyme Scheme\"}\n",
    "encoding_length = len(id2label)\n",
    "encoding_elements = id2label.items()\n",
    "\n",
    "def prepare_dataset(poems, classification_labels, reasoning_labels):\n",
    "    label_encoding = []\n",
    "    for classification_label, reasoning_label in zip(classification_labels, reasoning_labels):\n",
    "        labels = [0] * encoding_length\n",
    "        for index, label in encoding_elements:\n",
    "            if label == classification_label or label in reasoning_label.split(\", \"):\n",
    "                labels[index] = 1\n",
    "        label_encoding.append(labels)\n",
    "    return Dataset.from_dict({\"text\": [f\"Poem:\\n{p}\" for p in poems], \"label\": label_encoding})\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_length=256):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "questions_df = pd.read_csv(\"Copy of Fine_Tuning_Assignment - Limerick Classification.csv\")\n",
    "\n",
    "dataset = prepare_dataset(questions_df[\"Input (Poem)\"], questions_df[\"Label (Limerick or Non-Limerick)\"], questions_df[\"Reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aadbfb81-adcd-4741-884d-29a6ab58e8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0c3546ba-2302-471e-9033-1956057ce33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a55621cb81741748e4f4588f75bcd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We actually tokenize the dataset and then split the dataset into training and test sets (90-10 split)\n",
    "tokenized_dataset = dataset.map(lambda examples: tokenize_function(examples, tokenizer), batched=True)\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "91cab927-30ad-49d0-9de3-0e4f1169ea44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 540\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0728419d-19e6-4234-a87d-c75a754f1ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': \"Poem:\\nIf the Limerick's cocktail you 'd quaff,\\nStir nonsense with wit, each a half,\\nAdd a dash of good fun,\\nDrop in a pun-\\nAnd then make a noise like a laugh.\",\n",
       "  'label': [0, 1, 1, 0, 1, 0]},\n",
       " {'text': \"Poem:\\nIf the Limerick's cocktail you 'd quaff,\\nStir nonsense with wit, each a half,\\nAdd a dash of good fun,\\nDrop in a pun-\\nAnd then make a noise like a laugh.\",\n",
       "  'label': [0, 1, 1, 0, 1, 0],\n",
       "  'input_ids': [101,\n",
       "   5961,\n",
       "   1024,\n",
       "   2065,\n",
       "   1996,\n",
       "   15679,\n",
       "   1005,\n",
       "   1055,\n",
       "   18901,\n",
       "   2017,\n",
       "   1005,\n",
       "   1040,\n",
       "   24209,\n",
       "   10354,\n",
       "   2546,\n",
       "   1010,\n",
       "   16130,\n",
       "   14652,\n",
       "   2007,\n",
       "   15966,\n",
       "   1010,\n",
       "   2169,\n",
       "   1037,\n",
       "   2431,\n",
       "   1010,\n",
       "   5587,\n",
       "   1037,\n",
       "   11454,\n",
       "   1997,\n",
       "   2204,\n",
       "   4569,\n",
       "   1010,\n",
       "   4530,\n",
       "   1999,\n",
       "   1037,\n",
       "   26136,\n",
       "   1011,\n",
       "   1998,\n",
       "   2059,\n",
       "   2191,\n",
       "   1037,\n",
       "   5005,\n",
       "   2066,\n",
       "   1037,\n",
       "   4756,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'token_type_ids': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0], tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "625ad333-0cef-4c1d-bf37-da44a96ff51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use get_predictions to take in logits and then specifically set the indices corresponding to the highest value in the classification \n",
    "# indices (0, 1) to 1 and then the indices corresponding to the two highest values in the reasoning indices (2, 3, 4, 5) to 1 \n",
    "# and then return this.\n",
    "classification_indices = [0, 1]\n",
    "reasoning_indices = [2, 3, 4, 5]\n",
    "\n",
    "def get_predictions(input):\n",
    "    input_shape = input.shape\n",
    "    output = np.zeros(input_shape)\n",
    "    for i in range(len(input)):\n",
    "        top_class_index = np.argmax(input[i, classification_indices])\n",
    "        output[i, classification_indices[top_class_index]] = 1\n",
    "    for i in range(len(input)):\n",
    "        reasoning_values = input[i, reasoning_indices]\n",
    "        top_two_reasoning_indices = np.argsort(reasoning_values)[-2:]\n",
    "        for index in top_two_reasoning_indices:\n",
    "            output[i, reasoning_indices[index]] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d730274-d434-4721-ba28-523615cc4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute classification metrics (accuracy, precision, recall, and f1) using the sklearn library for both classification and reasoning parts\n",
    "def compute_metrics(predictions):\n",
    "    all_logits, all_labels = predictions\n",
    "    final_predictions = get_predictions(all_logits)\n",
    "    final_predictions = final_predictions.astype(int)\n",
    "    all_labels = all_labels.astype(int)\n",
    "    \n",
    "    classification_predictions = [np.argmax(row[classification_indices]) for row in final_predictions]\n",
    "    classification_labels = [np.argmax(row[classification_indices]) for row in all_labels]\n",
    "    classification_results = {\n",
    "        \"Classification Accuracy\": accuracy_score(classification_labels, classification_predictions),\n",
    "        \"Classification Precision\": precision_score(classification_labels, classification_predictions, average=\"binary\", zero_division=0),\n",
    "        \"Classification Recall\": recall_score(classification_labels, classification_predictions, average=\"binary\", zero_division=0),\n",
    "        \"Classification F1\": f1_score(classification_labels, classification_predictions, average=\"binary\", zero_division=0)\n",
    "    }\n",
    "\n",
    "    reasoning_predictions = final_predictions[:, reasoning_indices].flatten()\n",
    "    reasoning_labels = all_labels[:, reasoning_indices].flatten()\n",
    "    reasoning_results = {\n",
    "        \"Reasoning Accuracy\": accuracy_score(reasoning_labels, reasoning_predictions),\n",
    "        \"Reasoning Precision\": precision_score(reasoning_labels, reasoning_predictions, average=\"micro\", zero_division=0),\n",
    "        \"Reasoning Recall\": recall_score(reasoning_labels, reasoning_predictions, average=\"micro\", zero_division=0),\n",
    "        \"Reasoning F1\": f1_score(reasoning_labels, reasoning_predictions, average=\"micro\", zero_division=0)\n",
    "    }\n",
    "\n",
    "    return {**classification_results, **reasoning_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2559bb94-e09d-4853-824b-63ed16d6a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a custom trainer for binary class and multi-label/reason classification with weighted loss computations.\n",
    "classification_indices = [0, 1]\n",
    "reasoning_indices = [2, 3, 4, 5]\n",
    "\n",
    "class BinaryClassMultiLabelTrainer(Trainer):\n",
    "    def __init__(self, weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weights = weights\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        target_labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        predicted_logits = outputs[0]\n",
    "        classification_loss = torch.nn.functional.cross_entropy(predicted_logits[:, classification_indices], target_labels[:, classification_indices])\n",
    "        reasoning_loss = torch.nn.functional.binary_cross_entropy_with_logits(predicted_logits[:, reasoning_indices], target_labels[:, reasoning_indices])\n",
    "        loss = self.weights[0] * classification_loss + self.weights[1] * reasoning_loss\n",
    "        if return_outputs:\n",
    "            return (loss, outputs)\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10604c5c-3082-4220-8e55-fa346c335af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnapalempalli/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "output_dir = \"./fine_tuned_electra\"\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cd8a119c-2f04-4e82-919f-5c20e4a703ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 03:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Classification accuracy</th>\n",
       "      <th>Classification precision</th>\n",
       "      <th>Classification recall</th>\n",
       "      <th>Classification f1</th>\n",
       "      <th>Reasoning accuracy</th>\n",
       "      <th>Reasoning precision</th>\n",
       "      <th>Reasoning recall</th>\n",
       "      <th>Reasoning f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.425300</td>\n",
       "      <td>2.309034</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.760900</td>\n",
       "      <td>1.715516</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.672900</td>\n",
       "      <td>1.463872</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.294900</td>\n",
       "      <td>1.419877</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.371300</td>\n",
       "      <td>1.432367</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 1.432367205619812, 'eval_Classification Accuracy': 0.8, 'eval_Classification Precision': 0.6857142857142857, 'eval_Classification Recall': 0.96, 'eval_Classification F1': 0.7999999999999999, 'eval_Reasoning Accuracy': 0.875, 'eval_Reasoning Precision': 0.875, 'eval_Reasoning Recall': 0.875, 'eval_Reasoning F1': 0.875, 'eval_runtime': 0.9763, 'eval_samples_per_second': 61.459, 'eval_steps_per_second': 8.195, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# We initialize trainer\n",
    "trainer = BinaryClassMultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_test[\"train\"],\n",
    "    eval_dataset=train_test[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    weights=(0.7, 4)\n",
    ")\n",
    "trainer.can_return_loss = True\n",
    "\n",
    "# We start training\n",
    "trainer.train()\n",
    "\n",
    "# We save the trained model and evaluate the results\n",
    "trainer.save_model(\"./fine_tuned_electra\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_electra\")\n",
    "\n",
    "test_results = trainer.evaluate()\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "73d3cf54-0c2e-442e-ba0b-729bad4f5852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Predictions:\n",
      "Input: Poem:\n",
      "    A cannibal monarch imperial\n",
      "    Kept his wives on a diet of cereal,\n",
      "    But he didn't much care\n",
      "    What the women should wear,\n",
      "    Nor did they; it was quite immaterial.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a foppish old beau,\n",
      "    Who said, \"I find walking too sleau.\n",
      "    So I prances down the street\n",
      "    And throw out my feet\n",
      "    And trip my fantastical teau.\"\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a young maid from Japan\n",
      "    Who married a Hottentot man.\n",
      "    The girl she was yellow.\n",
      "    And black was the fellow.\n",
      "    And their children were all black and tan.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a poor fellow from Lynn,\n",
      "    By accident sat on a pynn,\n",
      "    He let out a shriek,\n",
      "    A howl and a squiek.\n",
      "    And his language was really a synn.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Professor, you should be commended\n",
      "    On your theory so geniusly splendid.\n",
      "    But some say it's luck,\n",
      "    And you really just suck,\n",
      "    'Cause your theory's not what you intended!\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a classical theory\n",
      "    Of which quantum disciples were leery.\n",
      "    They said, ‚ÄúWhy spend so long\n",
      "    On a theory that‚Äôs wrong?‚Äù\n",
      "    Well, it works for your everyday query!\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Consider, when seeking gestalts,\n",
      "    The theories that science exalts.\n",
      "    It's not that they're known\n",
      "    To be written in stone.\n",
      "    It's just that we can't say they're false.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    We need to take care of the one world we live in!\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    In familiar bed,\n",
      "    hands reaching into the light.\n",
      "    Soul blossoms tonight.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Prayers are good wishes\n",
      "    rising up to the realm of\n",
      "    possibilities.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Once more the storm is howling, and half hid\n",
      "    Under this cradle-hood and coverlid\n",
      "    My child sleeps on. There is no obstacle\n",
      "    But Gregory's wood and one bare hill\n",
      "    Whereby the haystack- and roof-levelling wind,\n",
      "    Bred on the Atlantic, can be stayed;\n",
      "    And for an hour I have walked and prayed\n",
      "    Because of the great gloom that is in my mind.\n",
      "    I have walked and prayed for this young child an hour\n",
      "    And heard the sea-wind scream upon the tower,\n",
      "    And under the arches of the bridge, and scream\n",
      "    In the elms above the flooded stream;\n",
      "    Imagining in excited reverie\n",
      "    That the future years had come,\n",
      "    Dancing to a frenzied drum,\n",
      "    Out of the murderous innocence of the sea.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    May she be granted beauty and yet not\n",
      "    Beauty to make a stranger's eye distraught,\n",
      "    Or hers before a looking-glass, for such,\n",
      "    Being made beautiful overmuch,\n",
      "    Consider beauty a sufficient end,\n",
      "    Lose natural kindness and maybe\n",
      "    The heart-revealing intimacy\n",
      "    That chooses right, and never find a friend.\n",
      "    Helen being chosen found life flat and dull\n",
      "    And later had much trouble from a fool,\n",
      "    While that great Queen, that rose out of the spray,\n",
      "    Being fatherless could have her way\n",
      "    Yet chose a bandy-legg√®d smith for man.\n",
      "    It's certain that fine women eat\n",
      "    A crazy salad with their meat\n",
      "    Whereby the Horn of Plenty is undone.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    A cannibal monarch\n",
      "    Kept his wives on a diet,\n",
      "    But he didn't much care\n",
      "    What the women should look like\n",
      "    Nor did they; it was quite immaterial.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a poor fellow,\n",
      "    By accident sat on a pynn,\n",
      "    He yelled out loud,\n",
      "    A howl and a squiek.\n",
      "    And his language was really a curse.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'AABBA Rhyme Scheme', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Fine-Tuned Model Predictions:\n",
      "Input: Poem:\n",
      "    A cannibal monarch imperial\n",
      "    Kept his wives on a diet of cereal,\n",
      "    But he didn't much care\n",
      "    What the women should wear,\n",
      "    Nor did they; it was quite immaterial.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a foppish old beau,\n",
      "    Who said, \"I find walking too sleau.\n",
      "    So I prances down the street\n",
      "    And throw out my feet\n",
      "    And trip my fantastical teau.\"\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a young maid from Japan\n",
      "    Who married a Hottentot man.\n",
      "    The girl she was yellow.\n",
      "    And black was the fellow.\n",
      "    And their children were all black and tan.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a poor fellow from Lynn,\n",
      "    By accident sat on a pynn,\n",
      "    He let out a shriek,\n",
      "    A howl and a squiek.\n",
      "    And his language was really a synn.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Professor, you should be commended\n",
      "    On your theory so geniusly splendid.\n",
      "    But some say it's luck,\n",
      "    And you really just suck,\n",
      "    'Cause your theory's not what you intended!\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a classical theory\n",
      "    Of which quantum disciples were leery.\n",
      "    They said, ‚ÄúWhy spend so long\n",
      "    On a theory that‚Äôs wrong?‚Äù\n",
      "    Well, it works for your everyday query!\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Consider, when seeking gestalts,\n",
      "    The theories that science exalts.\n",
      "    It's not that they're known\n",
      "    To be written in stone.\n",
      "    It's just that we can't say they're false.\n",
      "Output: ['Non-Limerick', '5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    We need to take care of the one world we live in!\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    In familiar bed,\n",
      "    hands reaching into the light.\n",
      "    Soul blossoms tonight.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Prayers are good wishes\n",
      "    rising up to the realm of\n",
      "    possibilities.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    Once more the storm is howling, and half hid\n",
      "    Under this cradle-hood and coverlid\n",
      "    My child sleeps on. There is no obstacle\n",
      "    But Gregory's wood and one bare hill\n",
      "    Whereby the haystack- and roof-levelling wind,\n",
      "    Bred on the Atlantic, can be stayed;\n",
      "    And for an hour I have walked and prayed\n",
      "    Because of the great gloom that is in my mind.\n",
      "    I have walked and prayed for this young child an hour\n",
      "    And heard the sea-wind scream upon the tower,\n",
      "    And under the arches of the bridge, and scream\n",
      "    In the elms above the flooded stream;\n",
      "    Imagining in excited reverie\n",
      "    That the future years had come,\n",
      "    Dancing to a frenzied drum,\n",
      "    Out of the murderous innocence of the sea.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    May she be granted beauty and yet not\n",
      "    Beauty to make a stranger's eye distraught,\n",
      "    Or hers before a looking-glass, for such,\n",
      "    Being made beautiful overmuch,\n",
      "    Consider beauty a sufficient end,\n",
      "    Lose natural kindness and maybe\n",
      "    The heart-revealing intimacy\n",
      "    That chooses right, and never find a friend.\n",
      "    Helen being chosen found life flat and dull\n",
      "    And later had much trouble from a fool,\n",
      "    While that great Queen, that rose out of the spray,\n",
      "    Being fatherless could have her way\n",
      "    Yet chose a bandy-legg√®d smith for man.\n",
      "    It's certain that fine women eat\n",
      "    A crazy salad with their meat\n",
      "    Whereby the Horn of Plenty is undone.\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    A cannibal monarch\n",
      "    Kept his wives on a diet,\n",
      "    But he didn't much care\n",
      "    What the women should look like\n",
      "    Nor did they; it was quite immaterial.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There was a poor fellow,\n",
      "    By accident sat on a pynn,\n",
      "    He yelled out loud,\n",
      "    A howl and a squiek.\n",
      "    And his language was really a curse.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are setting up the base version of the same model without the fine tuning for comparison purposes\n",
    "model_name = \"google/electra-base-discriminator\"\n",
    "finetuned_model_path = \"./fine_tuned_electra\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# We test the model on new examples that were not in our dataset\n",
    "new_test_examples = [\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    A cannibal monarch imperial\n",
    "    Kept his wives on a diet of cereal,\n",
    "    But he didn't much care\n",
    "    What the women should wear,\n",
    "    Nor did they; it was quite immaterial.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    There once was a foppish old beau,\n",
    "    Who said, \"I find walking too sleau.\n",
    "    So I prances down the street\n",
    "    And throw out my feet\n",
    "    And trip my fantastical teau.\"''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    There was a young maid from Japan\n",
    "    Who married a Hottentot man.\n",
    "    The girl she was yellow.\n",
    "    And black was the fellow.\n",
    "    And their children were all black and tan.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    There was a poor fellow from Lynn,\n",
    "    By accident sat on a pynn,\n",
    "    He let out a shriek,\n",
    "    A howl and a squiek.\n",
    "    And his language was really a synn.''',\n",
    "    #Limerick\n",
    "    '''Poem:\n",
    "    Professor, you should be commended\n",
    "    On your theory so geniusly splendid.\n",
    "    But some say it's luck,\n",
    "    And you really just suck,\n",
    "    'Cause your theory's not what you intended!''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    There once was a classical theory\n",
    "    Of which quantum disciples were leery.\n",
    "    They said, ‚ÄúWhy spend so long\n",
    "    On a theory that‚Äôs wrong?‚Äù\n",
    "    Well, it works for your everyday query!''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    Consider, when seeking gestalts,\n",
    "    The theories that science exalts.\n",
    "    It's not that they're known\n",
    "    To be written in stone.\n",
    "    It's just that we can't say they're false.''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    We need to take care of the one world we live in!''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    In familiar bed,\n",
    "    hands reaching into the light.\n",
    "    Soul blossoms tonight.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    Prayers are good wishes\n",
    "    rising up to the realm of\n",
    "    possibilities.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    Once more the storm is howling, and half hid\n",
    "    Under this cradle-hood and coverlid\n",
    "    My child sleeps on. There is no obstacle\n",
    "    But Gregory's wood and one bare hill\n",
    "    Whereby the haystack- and roof-levelling wind,\n",
    "    Bred on the Atlantic, can be stayed;\n",
    "    And for an hour I have walked and prayed\n",
    "    Because of the great gloom that is in my mind.\n",
    "    I have walked and prayed for this young child an hour\n",
    "    And heard the sea-wind scream upon the tower,\n",
    "    And under the arches of the bridge, and scream\n",
    "    In the elms above the flooded stream;\n",
    "    Imagining in excited reverie\n",
    "    That the future years had come,\n",
    "    Dancing to a frenzied drum,\n",
    "    Out of the murderous innocence of the sea.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    May she be granted beauty and yet not\n",
    "    Beauty to make a stranger's eye distraught,\n",
    "    Or hers before a looking-glass, for such,\n",
    "    Being made beautiful overmuch,\n",
    "    Consider beauty a sufficient end,\n",
    "    Lose natural kindness and maybe\n",
    "    The heart-revealing intimacy\n",
    "    That chooses right, and never find a friend.\n",
    "    Helen being chosen found life flat and dull\n",
    "    And later had much trouble from a fool,\n",
    "    While that great Queen, that rose out of the spray,\n",
    "    Being fatherless could have her way\n",
    "    Yet chose a bandy-legg√®d smith for man.\n",
    "    It's certain that fine women eat\n",
    "    A crazy salad with their meat\n",
    "    Whereby the Horn of Plenty is undone.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    A cannibal monarch\n",
    "    Kept his wives on a diet,\n",
    "    But he didn't much care\n",
    "    What the women should look like\n",
    "    Nor did they; it was quite immaterial.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There was a poor fellow,\n",
    "    By accident sat on a pynn,\n",
    "    He yelled out loud,\n",
    "    A howl and a squiek.\n",
    "    And his language was really a curse.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works'''\n",
    "]\n",
    "\n",
    "id2label={0: \"Non-Limerick\", 1: \"Limerick\", 2: \"5 Lines\", 3: \"Not 5 Lines\", 4: \"AABBA Rhyme Scheme\", 5: \"Not AABBA Rhyme Scheme\"}\n",
    "label2id={\"Non-Limerick\": 0, \"Limerick\": 1, \"5 Lines\": 2, \"Not 5 Lines\": 3, \"AABBA Rhyme Scheme\": 4, \"Not AABBA Rhyme Scheme\": 5}\n",
    "\n",
    "# We need to have the model make the predictions and then return these predictions as output\n",
    "def decode_predictions(model, tokenizer, new_test_examples, id2label):\n",
    "    tokenized_input = tokenizer(new_test_examples, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    predictions = get_predictions(model(**tokenized_input).logits.detach().numpy())\n",
    "    decoded_predictions = []\n",
    "    for row in predictions:\n",
    "        decoded_row = []\n",
    "        for i, label in enumerate(row):\n",
    "            if label == 1:\n",
    "                decoded_row.append(id2label[i])\n",
    "        decoded_predictions.append(decoded_row)\n",
    "    return decoded_predictions\n",
    "\n",
    "# Our baseline model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, id2label=id2label, label2id=label2id)\n",
    "# Our finetuned model\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "\n",
    "# We get predictions for both models\n",
    "base_model_predictions = decode_predictions(base_model, tokenizer, new_test_examples, id2label)\n",
    "finetuned_model_predictions = decode_predictions(finetuned_model, tokenizer, new_test_examples, id2label)\n",
    "\n",
    "# We print the results\n",
    "print(\"Base Model Predictions:\")\n",
    "for text, prediction in zip(new_test_examples, base_model_predictions):\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {prediction}\\n\")\n",
    "\n",
    "print(\"Fine-Tuned Model Predictions:\")\n",
    "for text, prediction in zip(new_test_examples, finetuned_model_predictions):\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b5ef874b-746a-479c-9edc-8898df71a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Predictions:\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Limerick', 'Not 5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Fine-Tuned Model Predictions:\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    God's first tries were hardly ideal,\n",
      "    You see, complex worlds have no appeal.\n",
      "    In the present edition,\n",
      "    He made things Hermitian,\n",
      "    And this world, it seems, is quite real.\n",
      "Output: ['Limerick', '5 Lines', 'AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n",
      "Input: Poem:\n",
      "    There once was a\n",
      "    Of which quantum.\n",
      "    They said,\n",
      "    On a theory\n",
      "    Well, it works\n",
      "Output: ['Non-Limerick', 'Not 5 Lines', 'Not AABBA Rhyme Scheme']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are setting up the base version of the same model without the fine tuning for comparison purposes\n",
    "model_name = \"google/electra-base-discriminator\"\n",
    "finetuned_model_path = \"./fine_tuned_electra\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# We test the model on new examples that were not in our dataset\n",
    "new_test_examples = [\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick:\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    #Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Limerick\n",
    "    '''Poem:\n",
    "    God's first tries were hardly ideal,\n",
    "    You see, complex worlds have no appeal.\n",
    "    In the present edition,\n",
    "    He made things Hermitian,\n",
    "    And this world, it seems, is quite real.''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "    # Non-Limerick\n",
    "    '''Poem:\n",
    "    There once was a\n",
    "    Of which quantum.\n",
    "    They said,\n",
    "    On a theory\n",
    "    Well, it works''',\n",
    "]\n",
    "\n",
    "id2label={0: \"Non-Limerick\", 1: \"Limerick\", 2: \"5 Lines\", 3: \"Not 5 Lines\", 4: \"AABBA Rhyme Scheme\", 5: \"Not AABBA Rhyme Scheme\"}\n",
    "label2id={\"Non-Limerick\": 0, \"Limerick\": 1, \"5 Lines\": 2, \"Not 5 Lines\": 3, \"AABBA Rhyme Scheme\": 4, \"Not AABBA Rhyme Scheme\": 5}\n",
    "\n",
    "# We need to have the model make the predictions and then return these predictions as output\n",
    "def decode_predictions(model, tokenizer, new_test_examples, id2label):\n",
    "    tokenized_input = tokenizer(new_test_examples, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    predictions = get_predictions(model(**tokenized_input).logits.detach().numpy())\n",
    "    decoded_predictions = []\n",
    "    for row in predictions:\n",
    "        decoded_row = []\n",
    "        for i, label in enumerate(row):\n",
    "            if label == 1:\n",
    "                decoded_row.append(id2label[i])\n",
    "        decoded_predictions.append(decoded_row)\n",
    "    return decoded_predictions\n",
    "\n",
    "# Our baseline model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, id2label=id2label, label2id=label2id)\n",
    "# Our finetuned model\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "\n",
    "# We get predictions for both models\n",
    "base_model_predictions = decode_predictions(base_model, tokenizer, new_test_examples, id2label)\n",
    "finetuned_model_predictions = decode_predictions(finetuned_model, tokenizer, new_test_examples, id2label)\n",
    "\n",
    "# We print the results\n",
    "print(\"Base Model Predictions:\")\n",
    "for text, prediction in zip(new_test_examples, base_model_predictions):\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {prediction}\\n\")\n",
    "\n",
    "print(\"Fine-Tuned Model Predictions:\")\n",
    "for text, prediction in zip(new_test_examples, finetuned_model_predictions):\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba8049-06e7-43c5-b063-d25b2e28b7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
